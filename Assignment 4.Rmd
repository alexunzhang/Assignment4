---
title: "Assignment 4"
author: "Alex Zhang"
date: "12/12/2021"
output: pdf_document
---

1. The business goal is to develop a prediction (classification) model for credit card fradulent transactions.

2. The dataset was retrieved from Kaggle (https://www.kaggle.com/mlg-ulb/creditcardfraud/version/3)

3.	identify the code that imported and saved your dataset in R 

```{r echo=TRUE}

library(tidyverse)
cc <- read_csv("C:/Users/alex_zhang4/Downloads/creditcard.csv")

```

4.	describe your data set (using the common attributes such as #rows, #columns, variable names, types, means, SD, min/max, NAs, etc...) 

```{r echo=TRUE}

dim(cc)
## The dataset has 31 columns and 284807 rows
spec(cc)
## the variables are Time, V1 to V28, and transaciton amount, and class (positive means Fraud)
table(cc$Class)
## out of total 284807 transactions, 492 are fraudulent
summary(cc)
## it shows the min, median, mean and max of each variable
var(cc$Amount)
sd(cc$Amount)
## it shows the variance and standard deviation of transaction amounts


```

5.	discuss any data preparation, missing values and errors 

```{r echo=TRUE}

## no missing value from the data. If there are missing value, then we need to understand the likely cause of missing values whether it's a random or not. If it's random, then the data while lacking statistical power may still be unbiased. If it's not random, then the data may be biased. Increasing the sample size will help address the problem. Also, we can use missing value deletion, mean or regression imputation to make up the missing values.

sum(is.na(cc))

## we will scale our data using the scale() function. We will apply this to the amount component of our creditcard_data amount. Scaling is also known as feature standardization. With the help of scaling, the data is structured according to a specified range. Therefore, there are no extreme values in our dataset that might interfere with the functioning of our model. 

cc$Amount = scale(cc$Amount)
newcc = cc[,-c(1)]
head(newcc)

```

6.	discuss the modeling

```{r echo=TRUE}

## After we have standardized our entire dataset, we will split our dataset into training set as well as test set with a split ratio of 0.80. This means that 80% of our data will be attributed to the train_data whereas 20% will be attributed to the test data.

library(caTools)
set.seed(123)
ccsample = sample.split(newcc$Class, SplitRatio = 0.8)
train_data = subset(newcc, ccsample == TRUE)
test_data = subset(newcc, ccsample == FALSE)

## We will begin with logistic regression. A logistic regression is used for modeling the outcome probability of a class such as pass/fail, positive/negative and in our case â€“ fraud/not fraud. 

cclog = glm(Class~.,train_data, family=binomial())
summary(cclog)

```

7.	produce and discuss the output 

```{r echo=TRUE}

## In order to assess the performance of our model, we will delineate the ROC curve. ROC is also known as Receiver Optimistic Characteristics. For this, we will first import the ROC package and then plot our ROC curve to analyze its performance.

library(pROC)
cc.predict <- predict(cclog, test_data, probability = TRUE)

ccpredchart <- roc(test_data$Class, cc.predict, plot = TRUE, col = "blue")

# Both Sensitivity and Specificity are high in this model. Sensitivity, the True Positive rate meaning the proportion of Fraudulent transactions that were predicted fraudent out of total Fraudent transactions, is between 90% to 100%. Specificity, the True Negative rate meaning the proportion of non-fraudent transactions that were predicted non-fraudelnt out of total non-fraudelent trasactions, is 100%. This indicates the model has strong prediction power.

```

8.	provide explanation with any visuals

```{r echo=TRUE}

plot(cclog)

## predicted values are very close to the fitted values


```
